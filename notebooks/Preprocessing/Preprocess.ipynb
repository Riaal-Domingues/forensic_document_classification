{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Preprocessing reference code\n",
    "\n",
    "The first cell below is the basic preprocessing as we call it for final preprocessing.  There is also a py script to perform the same.  This portion below is used for partial processing and as reference code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing experimental data\n",
    "\n",
    "The preprocessing code below is to generate data for preprocessing experiments. Standard preprocessing will remain in place for all emails:\n",
    "1. Remove standard email addresses\n",
    "1. Remove formatting including\n",
    "    1. Visual formatting (e.g. =02, =09, \\n\\n)\n",
    "    1. HTML tags\n",
    "1. Remove stopwords\n",
    "1. Expand contractions\n",
    "1. Lemmatize words (i.e. change them into their root word)\n",
    "1. Removal of single letters and possible double letter words.\n",
    "\n",
    "We are interested in the following experiments in preprocessing:\n",
    "\n",
    "1. The effect of leaving common names in an email.\n",
    "1. Special formats of email addresses that does not conform to external email address notation.\n",
    "1. Detecting single concept words and tying them together in the dictionary.\n",
    "1. Specialised tokenization.\n",
    "1. Filtering out \"Forwaded\" information from email bodies.\n",
    "\n",
    "The code that follows cover incrementally adding the filtering as described for the experiments.  The experiments are performed in relevant notebooks to the specific model under investigation.\n",
    "\n",
    "### Build a file list\n",
    "The first step is to build a file list in memory before commencing processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:56:02.066151Z",
     "iopub.status.busy": "2021-11-10T16:56:02.065482Z",
     "iopub.status.idle": "2021-11-10T16:56:07.561302Z",
     "shell.execute_reply": "2021-11-10T16:56:07.558376Z",
     "shell.execute_reply.started": "2021-11-10T16:56:02.066115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising the program\n",
      "Importing Spacy\n",
      "Loading encore web\n",
      "Initialisation of eflp complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [00:00, 19693.88it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from tqdm import tqdm   #To display progress bars\n",
    "\n",
    "# Import own defined functions and classes\n",
    "print(\"Initialising the program\")\n",
    "modules_path = os.path.join('..','..','src','modules','')\n",
    "sys.path.append(modules_path)\n",
    "import eflp\n",
    "import importlib\n",
    "importlib.reload(eflp)\n",
    "\n",
    "dir_list = [\"allen-p\",\"arnold-j\",\"arora-h\",\n",
    "            \"badeer-r\",\"bailey-s\",\"bass-e\",\n",
    "            \"baughman-d\",\"beck-s\",\"benson-r\",\n",
    "            \"blair-l\",\"brawner-s\",\"buy-r\",\n",
    "            \"campbell-l\",\"lay-k\",\"skilling-j\"]\n",
    "\n",
    "src_data_root = os.path.join(\"..\",\"..\",\"data\",\"raw\")\n",
    "#mail_src = os.path.join(src_data_root,\"maildir\",\"allen-p\",\"_sent_mail\")\n",
    "#mail_src = os.path.join(src_data_root,\"maildir\",\"allen-p\",\"all_documents\")\n",
    "mail_src = os.path.join(src_data_root,\"maildir\",\"allen-p\")\n",
    "\n",
    "\n",
    "email = eflp.Email_Forensic_Processor()\n",
    "\n",
    "# Define a helper function to construct the file list\n",
    "def build_file_list(src, type = \"\"):\n",
    "    # Initialise the list\n",
    "    src_dst = []\n",
    "    with tqdm() as pbar:\n",
    "        for dir_path, dirs, files in os.walk(src):\n",
    "            src_path = dir_path\n",
    "            for file in files:\n",
    "                #print(file)\n",
    "                if not re.search(r'^\\.',file):      # Ignore hidden files in Unix\n",
    "                    file_src_path = os.path.join(src_path,file)\n",
    "                    file_dst_path = file_src_path.replace(\"/raw/\",\"/processed/\")\n",
    "                    if type == \"experimental\":\n",
    "                        file_dst_path = file_dst_path.replace(\"/maildir/\",\"/experimental_data/\")\n",
    "                    file_dst_path = file_dst_path + \"json\"\n",
    "                    src_dst.append((file_src_path,file_dst_path))\n",
    "                    pbar.update(1)\n",
    "                        #print(\"   \",file_src_path,file_dst_path)\n",
    "    return src_dst\n",
    "\n",
    "\n",
    "def build_file_list_multex(filename, type = \"\"):\n",
    "    # Initialise the list\n",
    "    src_dst = []\n",
    "    file_list = open(src_data_root + \"/\" + filename,\"r\")\n",
    "    file_number = 0\n",
    "    \n",
    "    with tqdm() as pbar:\n",
    "        for file in file_list:\n",
    "            file_number = file_number + 1\n",
    "            src_dst.append((\"../../data/raw/\" + file.strip(),\"../../data/processed/Multex/Multex_\" + str(file_number) + \".json\"))\n",
    "    pbar.update(1)\n",
    "                        #print(\"   \",file_src_path,file_dst_path)\n",
    "    file_list.close()\n",
    "    return src_dst\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "src_dst = build_file_list(mail_src, type = \"experimental\")\n",
    "\n",
    "Multex_list = build_file_list_multex(\"Multex_files.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard full pre-process\n",
    "\n",
    "Run a standard pre-process on the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:22:27.210702Z",
     "iopub.status.busy": "2021-11-10T16:22:27.210229Z",
     "iopub.status.idle": "2021-11-10T16:28:35.884630Z",
     "shell.execute_reply": "2021-11-10T16:28:35.882179Z",
     "shell.execute_reply.started": "2021-11-10T16:22:27.210667Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [06:08,  8.23it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# Preprocess the emails and store them\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        if os.path.exists(file_pair[1]):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0])\n",
    "            email.saveMail(file_pair[1])\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very basic preprocess\n",
    "\n",
    "The very basic pre-process. This is achieved by a special call to the class to not invoke pre-processing, and then a manual call to the class to invoke a basic pre-process, followed by a call to the class to finalise the basic pre-process. The output filename is modified with a pre-prend string:\n",
    "\n",
    "very_basic_xxx.json\n",
    "\n",
    "Here, we only filter out html code, makes everything lower case, lemmatise and tokenise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:28:35.896103Z",
     "iopub.status.busy": "2021-11-10T16:28:35.894219Z",
     "iopub.status.idle": "2021-11-10T16:36:04.511310Z",
     "shell.execute_reply": "2021-11-10T16:36:04.508065Z",
     "shell.execute_reply.started": "2021-11-10T16:28:35.896001Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [07:28,  6.76it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"very_Basic_\"\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"very_basic\")\n",
    "            email.finalise_preprocess()\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic preprocess\n",
    "\n",
    "The basic pre-process.  This is achieved by a special call to the class to not invoke pre-processing, and then a manual call to the class to invoke a basic pre-process, followed by a call to the class to finalise the basic pre-process.  The output filename is modified with a pre-prend string:\n",
    "- basic_xxx.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:37:44.257915Z",
     "iopub.status.busy": "2021-11-10T16:37:44.256993Z",
     "iopub.status.idle": "2021-11-10T16:39:18.759456Z",
     "shell.execute_reply": "2021-11-10T16:39:18.758083Z",
     "shell.execute_reply.started": "2021-11-10T16:37:44.257876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [01:34, 32.11it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"Basic_\"\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"basic\")\n",
    "            email.finalise_preprocess()\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering special email addresses\n",
    "\n",
    "The internal email address representation is not standard.  A standard email is of the form:\n",
    "- name@domain.parts\n",
    "\n",
    "However, the javamailer seem to have an internal representation of the form:\n",
    "- name/domain/structure@company\n",
    "\n",
    "The below code performs basic pre-processing, and then additionally filters this special email address format using a specially crafted regular expression.  The filename is pre-pended with:\n",
    "- Mailer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:39:18.764132Z",
     "iopub.status.busy": "2021-11-10T16:39:18.763409Z",
     "iopub.status.idle": "2021-11-10T16:41:31.086304Z",
     "shell.execute_reply": "2021-11-10T16:41:31.084309Z",
     "shell.execute_reply.started": "2021-11-10T16:39:18.764085Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [02:12, 22.93it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"Mailer_\"\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"mailer\")\n",
    "            #email.remove_patterns(pattern_list = [eflp.EMAIL_ENRON,eflp.TWO_LETTERS])\n",
    "            email.finalise_preprocess()\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filtering names\n",
    "\n",
    "The name of the mailbox owner may dominate, en therefore should potentially be filtered out.  This is becuase all mails will either address the email box owner, or signed by the email box owner at the end.  Filtering of names may or may not be required, dependend on the final model.\n",
    "\n",
    "The file is pre-pended with:\n",
    "\n",
    "- Name_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:41:31.090744Z",
     "iopub.status.busy": "2021-11-10T16:41:31.089866Z",
     "iopub.status.idle": "2021-11-10T16:44:49.424641Z",
     "shell.execute_reply": "2021-11-10T16:44:49.423437Z",
     "shell.execute_reply.started": "2021-11-10T16:41:31.090688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [03:18, 15.30it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"Name_\"\n",
    "\n",
    "#Define a regular expression to filer the name.\n",
    "NAME_REGEX = \"[P,p]hillip|[A,a]llen\"\n",
    "NAME = re.compile(NAME_REGEX,flags=re.IGNORECASE)\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"name\")\n",
    "            #email.remove_patterns(pattern_list = [eflp.EMAIL_ENRON,eflp.TWO_LETTERS])\n",
    "            #email.remove_patterns(pattern_list = [NAME,eflp.TWO_LETTERS])\n",
    "            email.finalise_preprocess()\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Final full pre-process\n",
    "\n",
    "XXXX\n",
    "\n",
    "The file is pre-pended with:\n",
    "\n",
    "- Full_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T17:27:13.422036Z",
     "iopub.status.busy": "2021-11-10T17:27:13.419078Z",
     "iopub.status.idle": "2021-11-10T17:40:10.684266Z",
     "shell.execute_reply": "2021-11-10T17:40:10.680624Z",
     "shell.execute_reply.started": "2021-11-10T17:27:13.421931Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [12:57,  3.90it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"Full_\"\n",
    "\n",
    "#Define a regular expression to filer the name.\n",
    "#NAME_REGEX = \"[P,p]hillip|[A,a]llen\"\n",
    "#NAME = re.compile(NAME_REGEX,flags=re.IGNORECASE)\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"full\")\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T17:40:10.694246Z",
     "iopub.status.busy": "2021-11-10T17:40:10.693359Z",
     "iopub.status.idle": "2021-11-10T18:04:24.713110Z",
     "shell.execute_reply": "2021-11-10T18:04:24.712128Z",
     "shell.execute_reply.started": "2021-11-10T17:40:10.694210Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3034it [24:13,  2.09it/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Define a pre-pend to add to the file name so that experiments can extract the correct pre-processed files. \n",
    "pre_pend = \"POS_\"\n",
    "\n",
    "#Define a regular expression to filer the name.\n",
    "#NAME_REGEX = \"[P,p]hillip|[A,a]llen\"\n",
    "#NAME = re.compile(NAME_REGEX,flags=re.IGNORECASE)\n",
    "\n",
    "with tqdm(total=(len(src_dst) - 1)) as pbar:\n",
    "    for file_pair in src_dst:\n",
    "        # Construct a new output file name.\n",
    "        output_file = os.path.join(os.path.dirname(file_pair[1]),pre_pend + os.path.basename(file_pair[1]))\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"full_pos\")\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for additional tests\n",
    "\n",
    "The code that follows creates subset datasets for additional testing of LDA concepts. The main theory is that by controlling the dictionary with a careful seelction of subsets of data, the LDA algorithm will be better \"tuned\" on specific topics. The full pre-process is run on the subsets in preparation for the LDA algorithm to be applied.\n",
    "\n",
    "### Newsletter\n",
    "The Multex newsletter was identified to be of specific topics. The filenames were extracted by a simple unix command search and saved in Multex_filenames.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T16:50:20.596246Z",
     "iopub.status.busy": "2021-11-10T16:50:20.595224Z",
     "iopub.status.idle": "2021-11-10T16:50:20.647887Z",
     "shell.execute_reply": "2021-11-10T16:50:20.644766Z",
     "shell.execute_reply.started": "2021-11-10T16:50:20.596138Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "149it [00:00, 6289.32it/s]                                                      \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=(len(Multex_list) - 1)) as pbar:\n",
    "    for file_pair in Multex_list:\n",
    "        # Construct a new output file name.\n",
    "        output_file = file_pair[1]\n",
    "        if os.path.exists(output_file):\n",
    "            #print(\"file exists\")\n",
    "            #print(output_file)\n",
    "            pass\n",
    "        else:\n",
    "            #print(file_pair[0])\n",
    "            email.initMail(file_pair[0], preProcess = False)\n",
    "            email.preProcess(type=\"full\")\n",
    "            email.saveMail(output_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "_________________________________________________________\n",
    "# End Notebook\n",
    "________________\n",
    "\n",
    "\n",
    "## Special experimentation section\n",
    "\n",
    "In the section below we perform general experiments with code.  This will be deleted in the final notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-27T09:21:45.647974Z",
     "iopub.status.busy": "2021-08-27T09:21:45.647147Z",
     "iopub.status.idle": "2021-08-27T09:21:45.678347Z",
     "shell.execute_reply": "2021-08-27T09:21:45.676776Z",
     "shell.execute_reply.started": "2021-08-27T09:21:45.647930Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeff,\n",
      "\n",
      "I am not willing to guarantee to refinance the 1st lien on the stage in 4 \n",
      "years and drop the rate on both notes at that point to 8%.  There are several \n",
      "reasons that I won't commit to this.  Exposure to interest fluctuations, the \n",
      "large cash reserves needed, and the limited financial resources of the buyer \n",
      "are the three biggest concerns.  \n",
      "\n",
      "What I am willing to do is lower the second note to 8% amortized over the \n",
      "buyers choice of terms up to 30 years.  The existing note does not come due \n",
      "until September 2009.  That is a long time.  The buyer may have sold the \n",
      "property.  Interest rates may be lower.  I am bending over backwards to make \n",
      "the deal work with such an attractive second note.  Guaranteeing to refinance \n",
      "is pushing too far.\n",
      "\n",
      "Can you clarify the dates in the contract.  Is the effective date the day the \n",
      "earnest money is receipted or is it once the feasibility study is complete?\n",
      "\n",
      "Hopefully the buyer can live with these terms.  I got your fax from the New \n",
      "Braunfels buyer.  If we can't come to terms with the first buyer I will get \n",
      "started on the list.\n",
      "\n",
      "Email or call  me later today.\n",
      "\n",
      "Phillip\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "################ Function definitions ###############\n",
    "# A helper function which loads the json email and returns it as a dictionary\n",
    "def loadMail(filename):\n",
    "    with open (filename, \"r\") as inputFile:\n",
    "        return json.load(inputFile)\n",
    "################ End Function definitions ###############\n",
    "\n",
    "email = loadMail(\"../../data/processed/experimental_data/allen-p/_sent_mail/524.json\")\n",
    "#print(email)\n",
    "\n",
    "FORMATTING_REGEX = r\"=\\n|=\\d+\"\n",
    "\n",
    "EMAIL_REGEX = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "\n",
    "ENRON_EMAIL_REGEX = r\"/\\w+/\\w+/\\w+@\\w+|/\\w+/\\w+@\\w+|/\\w+@\\w+|/\\w+/Enron Communications@Enron Communication|/HOU/ECT|/NA/Enron|@ENRON\"\n",
    "\n",
    "\n",
    "NAME_REGEX_1 = \"[P,p]hillip\"\n",
    "NAME_REGEX_2 = \"[A,a]llen k\"\n",
    "NAME_REGEX_3 = \"[A,a]llen.\"\n",
    "\n",
    "HTML_REGEX = r\"<.*?>\"\n",
    "CHARACTERS_REGEX = r\"[,_]\"\n",
    "BELONG_REGEX = r\"'s\"\n",
    "TWO_LETTERS_REGEX = r\"\\b[\\w]{1,2}\\b\"\n",
    "NEWLINE_WORD_REGEX = r\"[\\n](?=\\w)\"\n",
    "PARAGRAPH_REGEX = r\"[\\n]\"\n",
    "\n",
    "#print(FORMATTING_REGEX)\n",
    "FORMATTING = re.compile(FORMATTING_REGEX,flags=re.IGNORECASE)\n",
    "EMAIL = re.compile(EMAIL_REGEX,flags=re.IGNORECASE)\n",
    "EMAIL_ENRON = re.compile(ENRON_EMAIL_REGEX,flags=re.IGNORECASE)\n",
    "HTML = re.compile(HTML_REGEX,flags = re.IGNORECASE)\n",
    "CHARACTERS = re.compile(CHARACTERS_REGEX,flags = re.IGNORECASE)\n",
    "TWO_LETTERS = re.compile(TWO_LETTERS_REGEX,flags = re.IGNORECASE)\n",
    "URL = re.compile(URL_REGEX, flags = re.IGNORECASE)\n",
    "NEWLINE_WORD = re.compile(NEWLINE_WORD_REGEX, flags = re.IGNORECASE)\n",
    "PARAGRAPH = re.compile(PARAGRAPH_REGEX, flags = re.IGNORECASE)\n",
    "\n",
    "\n",
    "def remove_patterns(text,pattern_list = None, flags = None):\n",
    "    \n",
    "    if pattern_list == None:\n",
    "        return(text)\n",
    "    else:\n",
    "        for pattern in pattern_list:\n",
    "            text = pattern.sub('',text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "\n",
    "#print(email.keys())\n",
    "print(email['body'])\n",
    "pattern_list = [FORMATTING,EMAIL,URL,EMAIL_ENRON,NEWLINE_WORD]\n",
    "\n",
    "#sample = \"This is a example of an sentence with two and one letter s s s\"\n",
    "#sample_url = \"http://www.up.za\"\n",
    "#print()\n",
    "#print(TWO_LETTERS.sub('',sample))\n",
    "processed_body = remove_patterns(email['body'],pattern_list = pattern_list)\n",
    "#processed_body = remove_patterns(email.body,pattern_list = pattern_list)\n",
    "#print(\"\\n\\n\")\n",
    "#print(processed_body)\n",
    "#print(\"\\n\\n\\n\")\n",
    "processed_body\n",
    "paragraphs = re.split(PARAGRAPH,processed_body)\n",
    "paragraphs = list(filter(''.__ne__,paragraphs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:15:35.099241Z",
     "iopub.status.busy": "2021-08-01T05:15:35.098777Z",
     "iopub.status.idle": "2021-08-01T05:15:35.153229Z",
     "shell.execute_reply": "2021-08-01T05:15:35.150367Z",
     "shell.execute_reply.started": "2021-08-01T05:15:35.099207Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a link to the governor's press release.  He is billing it as 5,000 MW of contracts, but then he says that there is only 500 available immediately.  WIth the remainder available from 3 to 10 years.\n",
      "http://www.governor.ca.gov/state/govsite/gov_htmldisplay.jsp?BV_SessionID=@@@@1673762879.0981503886@@@@&BV_EngineID=falkdgkgfmhbemfcfkmchcng.0&sCatTitle=Press+Release&sFilePath=/govsite/press_release/2001_02/20010206_PR01049_longtermcontracts.html&sTitle=GOVERNOR+DAVIS+ANNOUNCES+LONG+TERM+POWER+SUPPLY&iOID=13250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "################ Function definitions ###############\n",
    "# A helper function which loads the json email and returns it as a dictionary\n",
    "def loadMail(filename):\n",
    "    with open (filename, \"r\") as inputFile:\n",
    "        return json.load(inputFile)\n",
    "################ End Function definitions ###############\n",
    "\n",
    "email = loadMail(\"../../data/processed/maildir/allen-p/_sent_mail/520.json\")\n",
    "#email = loadMail(\"../../data/processed/maildir/allen-p/_sent_mail/2.json\")\n",
    "\n",
    "\n",
    "\n",
    "FORMATTING_REGEX = r\"=\\n|=\\d+\"\n",
    "\n",
    "EMAIL_REGEX = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
    "URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "\n",
    "ENRON_EMAIL_REGEX = r\"/\\w+/\\w+/\\w+@\\w+|/\\w+/\\w+@\\w+|/\\w+@\\w+|/\\w+/Enron Communications@Enron Communication|/HOU/\\w+|/NA/Enron|@ENRON\"\n",
    "\n",
    "\n",
    "NAME_REGEX_1 = \"[P,p]hillip\"\n",
    "NAME_REGEX_2 = \"[A,a]llen k\"\n",
    "NAME_REGEX_3 = \"[A,a]llen.\"\n",
    "\n",
    "NAME_REGEX = \"[P,p]hillip|[A,a]llen\"\n",
    "NAME = re.compile(NAME_REGEX, flags = re.IGNORECASE)\n",
    "\n",
    "HTML_REGEX = r\"<.*?>\"\n",
    "CHARACTERS_REGEX = r\"[,_]\"\n",
    "BELONG_REGEX = r\"'s\"\n",
    "TWO_LETTERS_REGEX = r\"\\b[\\w]{1,2}\\b\"\n",
    "\n",
    "#print(FORMATTING_REGEX)\n",
    "FORMATTING = re.compile(FORMATTING_REGEX,flags=re.IGNORECASE)\n",
    "EMAIL = re.compile(EMAIL_REGEX,flags=re.IGNORECASE)\n",
    "EMAIL_ENRON = re.compile(ENRON_EMAIL_REGEX,flags=re.IGNORECASE)\n",
    "HTML = re.compile(HTML_REGEX,flags = re.IGNORECASE)\n",
    "CHARACTERS = re.compile(CHARACTERS_REGEX,flags = re.IGNORECASE)\n",
    "TWO_LETTERS = re.compile(TWO_LETTERS_REGEX,flags = re.IGNORECASE)\n",
    "URL = re.compile(URL_REGEX, flags = re.IGNORECASE)\n",
    "\n",
    "\n",
    "def remove_patterns(text,pattern_list = None, flags = None):\n",
    "    \n",
    "    if pattern_list == None:\n",
    "        return(text)\n",
    "    else:\n",
    "        for pattern in pattern_list:\n",
    "            text = pattern.sub('',text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "def remove_justify(text):\n",
    "    new_text = \"\"\n",
    "    lines = re.findall(r\".+\\n\", text, flags=0)\n",
    "    for line in lines:\n",
    "        if len(line) == 79:\n",
    "            #print(repr(line))\n",
    "            line = re.sub(r\"\\n\",\"\",line)\n",
    "            #print(repr(line))\n",
    "        elif len(line) == 77:\n",
    "            line = re.sub(r\"=\\n\",\"\",line)\n",
    "        elif len(line) == 75:\n",
    "            line = re.sub(r\"\\n\",\"\",line)\n",
    "\n",
    "        new_text += line\n",
    "        #print(len(line))\n",
    "        #print(repr(line))\n",
    "    return(new_text)\n",
    "\n",
    "def remove_forward(text):\n",
    "    if re.search(r\"(-+ Forwarded by .+Subject:)\",text,flags=re.DOTALL) == None:\n",
    "        #print(re.search(r\"(-+ Forwarded by .+Subject:)\",text))\n",
    "        pass\n",
    "    else:\n",
    "        #print(\"Removing forward\")\n",
    "        text = re.sub(r\"(-+ Forwarded by .+Subject:)\",\"\",text,flags=re.DOTALL)\n",
    "        text = re.sub(r\"^.+\\n\",\"\",text)\n",
    "    return text\n",
    "\n",
    " \n",
    "#print(email['body'])\n",
    "cleaned = remove_justify(email['body'])\n",
    "#print(re.search(r\"-+ Forwarded by .+\",cleaned))\n",
    "#print(cleaned)\n",
    "#print(re.search(r\"(-+ Forwarded by)\",cleaned))\n",
    "cleaned = remove_forward(cleaned)\n",
    "print(cleaned)\n",
    "\n",
    "#print(cleared)\n",
    "\n",
    "                     \n",
    "#print(email['body'])\n",
    "#pattern_list = [FORMATTING,EMAIL,URL,EMAIL_ENRON]\n",
    "#pattern_list = [URL]\n",
    "\n",
    "#url = \"http://www.governor.ca.gov/state/govsite/gov_htmldisplay.jsp?BV_SessionID=@@@@1673762879.0981503886@@@@&BV_EngineID=falkdgkgfmhbemfcfkmchcng.0&sCatTitle=Press+Release&sFilePath=/govsite/press_release/2001_02/20010206_PR01049_longtermcontracts.html&sTitle=GOVERNOR+DAVIS+ANNOUNCES+LONG+TERM+POWER+SUPPLY&iOID=13250\"\n",
    "#print(url)\n",
    "#print(remove_patterns(url, pattern_list = pattern_list))\n",
    "#sample = \"This is a example of an sentence with two and one letter s s s\"\n",
    "#sample_url = \"http://www.up.za\"\n",
    "#print()\n",
    "#print(TWO_LETTERS.sub('',sample))\n",
    "#processed_body = remove_patterns(email['body'],pattern_list = pattern_list)\n",
    "#processed_body = remove_patterns(email.body,pattern_list = pattern_list)\n",
    "#print(\"\\n\\n\")\n",
    "#print(processed_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:15:44.182981Z",
     "iopub.status.busy": "2021-08-01T05:15:44.182465Z",
     "iopub.status.idle": "2021-08-01T05:15:44.195530Z",
     "shell.execute_reply": "2021-08-01T05:15:44.193430Z",
     "shell.execute_reply.started": "2021-08-01T05:15:44.182945Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_specification = [\n",
    "    ('NUMBER',   r'\\d+(\\.\\d*)?'),  # Integer or decimal number\n",
    "    ('ASSIGN',   r':='),           # Assignment operator\n",
    "    ('END',      r';'),            # Statement terminator\n",
    "    ('ID',       r'[A-Za-z]+'),    # Identifiers\n",
    "    ('OP',       r'[+\\-*/]'),      # Arithmetic operators\n",
    "    ('NEWLINE',  r'\\n'),           # Line endings\n",
    "    ('SKIP',     r'[ \\t]+'),       # Skip over spaces and tabs\n",
    "    ('MISMATCH', r'.'),            # Any other character\n",
    "]\n",
    "tok_regex = '|'.join('(?P<%s>%s)' % pair for pair in token_specification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-01T05:15:48.671422Z",
     "iopub.status.busy": "2021-08-01T05:15:48.670278Z",
     "iopub.status.idle": "2021-08-01T05:15:48.687993Z",
     "shell.execute_reply": "2021-08-01T05:15:48.682567Z",
     "shell.execute_reply.started": "2021-08-01T05:15:48.671363Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NUMBER', '\\\\d+(\\\\.\\\\d*)?'), ('ASSIGN', ':='), ('END', ';'), ('ID', '[A-Za-z]+'), ('OP', '[+\\\\-*/]'), ('NEWLINE', '\\\\n'), ('SKIP', '[ \\\\t]+'), ('MISMATCH', '.')]\n",
      "(?P<NUMBER>\\d+(\\.\\d*)?)|(?P<ASSIGN>:=)|(?P<END>;)|(?P<ID>[A-Za-z]+)|(?P<OP>[+\\-*/])|(?P<NEWLINE>\\n)|(?P<SKIP>[ \\t]+)|(?P<MISMATCH>.)\n"
     ]
    }
   ],
   "source": [
    "print(token_specification)\n",
    "print(tok_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
