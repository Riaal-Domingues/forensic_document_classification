{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc66a23-778c-42d9-beab-945fd6b19b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T19:55:18.174850Z",
     "iopub.status.busy": "2021-11-11T19:55:18.173592Z",
     "iopub.status.idle": "2021-11-11T19:55:21.882134Z",
     "shell.execute_reply": "2021-11-11T19:55:21.880931Z",
     "shell.execute_reply.started": "2021-11-11T19:55:18.174698Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import os\n",
    "import tempfile\n",
    "import math\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "################ Function definitions ###############\n",
    "# A helper function which loads the json email and returns it as a dictionary\n",
    "def loadMail(filename):\n",
    "    with open (filename, \"r\") as inputFile:\n",
    "        return json.load(inputFile)\n",
    "################ End Function definitions ###############\n",
    "\n",
    "\n",
    "########## Code: Loading Dictionaries #########\n",
    "def build_dictionaries(subdir, pre_pend_filter = \"\",  bow_docs_pos = None, dictionary_pos = None, texts_pos = None, files = None, limit = 10000, paragraphs = False):\n",
    "    # This technique requires dictionaries to fit in available memory.\n",
    "    # It is good for experimenting, but not for general purpose use if lots of data will be processed.\n",
    "    if(bow_docs_pos == None):\n",
    "        bow_docs_pos = []\n",
    "        texts_pos = []\n",
    "        dictionary_pos = Dictionary()      # Dictionary based on Part of Speech tagging\n",
    "        files = []\n",
    "    bow_docs_common = []\n",
    "    texts_common = []\n",
    "    dictionary_common = Dictionary()   # Dictionary based on common words\n",
    "\n",
    "\n",
    "    print(\"Building dictionaries\\n\")\n",
    "\n",
    "    # Build a dictionary of all the emails\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(subdir):\n",
    "        for file in files:\n",
    "            if not re.search(r'^\\.',file):     #Filter out common files created by the OS\n",
    "                filename = os.path.basename(file)\n",
    "                if pre_pend_filter == \"\" or re.match(pre_pend_filter, filename, flags=re.IGNORECASE) != None:\n",
    "                    email_file = os.path.join(root,file)\n",
    "                    #new_mail.initMail(email_file)\n",
    "                    # Build a dictionary and BOW from common token documents\n",
    "\n",
    "                    #for key, value in developer.items():\n",
    "                        #print(key, \":\", value)\n",
    "                    email_dict = loadMail(email_file)\n",
    "                    if paragraphs:\n",
    "                        paragraph_list = email_dict['body_pos_paragraph_tokens']\n",
    "                        if len(paragraph_list) > 0:\n",
    "                            for paragraph in paragraph_list:\n",
    "                                texts_pos.append(paragraph)\n",
    "                                dictionary_pos.add_documents([paragraph])\n",
    "                                bow_docs_pos.append(dictionary_pos.doc2bow(paragraph))\n",
    "                        files.append(email_file)\n",
    "                                \n",
    "                    else:\n",
    "                        texts_common.append(email_dict['body_tokens'])\n",
    "                        dictionary_common.add_documents([email_dict['body_tokens']])\n",
    "                        bow_docs_common.append(dictionary_common.doc2bow(email_dict[\"body_tokens\"]))\n",
    "\n",
    "                        # Build a specialised POS dictionary and BOW\n",
    "                        if(email_dict[\"body_pos_tokens\"] != None):\n",
    "                            texts_pos.append(email_dict[\"body_pos_tokens\"])\n",
    "                            dictionary_pos.add_documents([email_dict[\"body_pos_tokens\"]])\n",
    "                            bow_docs_pos.append(dictionary_pos.doc2bow(email_dict[\"body_pos_tokens\"]))\n",
    "                        files.append(email_file)\n",
    "\n",
    "                    if count == limit:\n",
    "                        break\n",
    "                    else:\n",
    "                        count = count + 1\n",
    "    print(\"Finished building dictionaries.\\n\\n\")\n",
    "\n",
    "    print(\"Number of documents in the BOW corpus: \",len(bow_docs_common))\n",
    "    print(\"Number of documents in the POS corpus: \",len(bow_docs_pos))\n",
    "    \n",
    "    return bow_docs_common,dictionary_common,texts_common,bow_docs_pos,dictionary_pos,texts_pos,files\n",
    "\n",
    "### Define a helper function to initialise and train the LDA model #####\n",
    "### The defaults fix some hyperparameters for the LDA model which can be overwritten #####\n",
    "def build_LDA_model(dictionary, bow_docs, num_topics = 2, \n",
    "              chunksize = 2000, \n",
    "              passes = 20, \n",
    "              iterations = 400, \n",
    "              eval_every = None):\n",
    "\n",
    "    dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    model = LdaModel(\n",
    "        corpus=bow_docs,\n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Define a general helper function to take the trained model and plot a wordcloud of the topics.\n",
    "def plot_wordcloud(model,filename = ''):\n",
    "    # Extract the topic words and related frequecies.\n",
    "    topics = model.show_topics(formatted=False)\n",
    "\n",
    "    # Define a square mask to print the wordcloud.\n",
    "    x, y = np.ogrid[:500, :500]\n",
    "    mask = (x - 250) ** 2 + (y - 250) ** 2 > 250 ** 2\n",
    "    mask = 255 * mask.astype(int)\n",
    "\n",
    "    # For each topic, extract the word and frequencies, build a wordcloud and display it.\n",
    "    for topic_nr,topic in zip(range(0,len(topics)),topics):\n",
    "        topic_words = dict(topic[1])\n",
    "        cloud = WordCloud(mask=mask,background_color=\"white\").generate_from_frequencies(topic_words)\n",
    "        plt.figure()\n",
    "        plt.imshow(cloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Topic nr : \" + str(topic_nr + 1))\n",
    "        plt.show()\n",
    "        if filename != '':\n",
    "            topic_filename = filename + '_topic_' + str(topic_nr + 1) + '.png'\n",
    "            print('Saving: ',topic_filename)\n",
    "            cloud.to_file(topic_filename)\n",
    "            \n",
    "# Helper function to score topics in a range and retain the models in memory\n",
    "def score_topics(dictionary, bow_docs, texts, start=1, stop=2, step=1, show_progress = True,\n",
    "                store_models = False, prefix='', num_topics_list = None):\n",
    "\n",
    "    scores = []\n",
    "    if num_topics_list is None:\n",
    "        num_topics_list = list(range(start,stop))   # Build a list of integers from start to stop\n",
    "        num_topics_subset = num_topics_list[::step] # Pick the subset with step size\n",
    "    else:\n",
    "        num_topics_subset = num_topics_list\n",
    "        \n",
    "  \n",
    "        \n",
    "    with tqdm(total=len(num_topics_subset)) as pbar:\n",
    "        for num_topics in num_topics_subset:\n",
    "            model = build_LDA_model(dictionary, bow_docs, num_topics)   #Build the model\n",
    "            coherence_model_lda = CoherenceModel(model=model, texts = texts, \n",
    "                                                 dictionary = dictionary, \n",
    "                                                 coherence='c_v')       # Build the coherence model\n",
    "            coherence_lda = coherence_model_lda.get_coherence()         # Get the coherence score\n",
    "            scores.append((num_topics, coherence_lda, model))           # Save the coherece score with the associated number of topics\n",
    "            #if store_models:\n",
    "            #    with tempfile.NamedTemporaryFile(mode='w+b', prefix=prefix + '_'+str(num_topics)+'_',\n",
    "            #                                     dir=tmpdir,delete=False) as tmp:\n",
    "            #        lda.save(tmp.name)\n",
    "            pbar.update(1)\n",
    "    \n",
    "    if store_models:\n",
    "        tmpdir = tempfile.mkdtemp(prefix = prefix + '_',dir = '.')     #Create a directory to save the models\n",
    "        score_pairs = {}\n",
    "        for score in scores:\n",
    "            num_topics,score,model = score       #Extract the three elements\n",
    "            with tempfile.NamedTemporaryFile(mode='w+b', prefix=prefix + '_'+str(num_topics)+'_',\n",
    "                                             dir=tmpdir,delete=False) as tmp:\n",
    "                    model.save(tmp.name)\n",
    "            score_pairs[num_topics] = score\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', prefix=prefix + '_scores_', suffix = '.json',\n",
    "                                         dir=tmpdir,delete=False) as tmp:\n",
    "            print(score_pairs)\n",
    "            json.dump(score_pairs,tmp)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Helper function which uses plotly to plot the scores\n",
    "def plot_topic_scores(scores,reference = 0):\n",
    "    x = []\n",
    "    y = []\n",
    "    min = 1\n",
    "    max = 0\n",
    "\n",
    "    # Extract the scores as x,y pairs\n",
    "    for entry in scores:\n",
    "        x.append(entry[0])\n",
    "        y.append(entry[1])\n",
    "        if entry[1] > max:\n",
    "            max = entry[1]\n",
    "        if entry[1] < min:\n",
    "            min = entry[1];\n",
    "\n",
    "    fig = go.Figure(data=go.Scatter(x=x, y=y,name='Scores'))\n",
    "    if reference != 0:\n",
    "        fig.add_scatter(x=[reference]*100, y=np.linspace(min,max,100), name='Reference point')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorise(topic_scores,nr_topics):\n",
    "    vector = [0] * nr_topics\n",
    "    \n",
    "    for score in topic_scores:\n",
    "        vector[score[0]] = score[1]\n",
    "    return vector\n",
    "\n",
    "def dot_product(vector1,vector2):\n",
    "    dot_product_sum = 0\n",
    "    vector1_sum = 0\n",
    "    vector2_sum = 0\n",
    "    \n",
    "    for v1,v2 in zip(vector1,vector2):\n",
    "        dot_product_sum = dot_product_sum + (v1*v2)\n",
    "        vector1_sum = vector1_sum + v1*v1\n",
    "        vector2_sum = vector2_sum + v2*v2\n",
    "        \n",
    "    vector_product = round(dot_product_sum,20) / round(math.sqrt(vector1_sum) * math.sqrt(vector2_sum),20)\n",
    "    return (round(vector_product,5))\n",
    "\n",
    "def cosine_sim(vector1,vector2):\n",
    "    vector_product = dot_product(vector1,vector2)\n",
    "    try:\n",
    "        angle_radians = math.acos(vector_product)\n",
    "    except:\n",
    "        print(vector_product)\n",
    "        if(vector_product > 1.0):\n",
    "            vector_product = 1.0\n",
    "            angle_radians = math.acos(vector_product)\n",
    "    return math.degrees(angle_radians)\n",
    "\n",
    "def simialarity(score1,score2,nr_topics):\n",
    "    vector1 = vectorise(score1,nr_topics)\n",
    "    vector2 = vectorise(score2,nr_topics)\n",
    "    angle = cosine_sim(vector1,vector2)\n",
    "    percentage_simialarity = 100 * (90-angle) / 90\n",
    "    return percentage_simialarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f804bef5-ccf6-4e63-99a3-e60fd5c9608d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-11T19:55:21.889481Z",
     "iopub.status.busy": "2021-11-11T19:55:21.885044Z",
     "iopub.status.idle": "2021-11-11T19:55:22.968867Z",
     "shell.execute_reply": "2021-11-11T19:55:22.967156Z",
     "shell.execute_reply.started": "2021-11-11T19:55:21.889426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Multex dictionaries\n",
      "Building dictionaries\n",
      "\n",
      "Finished building dictionaries.\n",
      "\n",
      "\n",
      "Number of documents in the BOW corpus:  149\n",
      "Number of documents in the POS corpus:  149\n"
     ]
    }
   ],
   "source": [
    "########## Initialisation Section ##########\n",
    "# Set the location of the directory used for processing\n",
    "maildir_path = os.path.join('..','..','data', 'processed', 'experimental_data')\n",
    "subdir = os.path.join(maildir_path,'allen-p') \n",
    "#subdir = os.path.join(maildir_path,'allen-p','_sent_mail') \n",
    "\n",
    "multex_subdir = os.path.join('..','..','data', 'processed', 'Multex')\n",
    "\n",
    "\n",
    "\n",
    "########## End Initialisation Section ##########\n",
    "\n",
    "\n",
    "\n",
    "print('Building Multex dictionaries')\n",
    "_,_,_,multex_bow,multex_dictionary,multex_texts,multex_files = build_dictionaries(multex_subdir,pre_pend_filter = \"\", paragraphs=False)\n",
    "#_,_,_,full_bow,full_dictionary,full_texts,full_files = build_dictionaries(subdir,pre_pend_filter = \"Full_\", paragraphs=False, bow_docs_pos = multex_bow, dictionary_pos = email_dictionary, texts_pos = email_texts, files = email_files)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c3643-b954-44b4-9b34-1dae094e3181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
